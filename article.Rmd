---
title: "The Practical Application of MST"
author:
- name: James Page
  affiliation: SP Educational Consulting
  affiliation_url: https://www.speduconsulting.co.uk/
date: "`r Sys.Date()`"
output:
  word_document: default
  pdf_document:
    highlight: tango
    latex_engine: lualatex
  html_document:
    theme: cerulean
    highlight: tango
version: 1
description: |
  A practical guide to the design and development of computerised multistage tests.
bibliography: references.bib
---

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)

library(mstR)
library(ltm)
```

# Introduction and Overview

Computerised multistage tests (MSTs) have grown in popularity in recent years due to their advantages of flexibility and simplicity over linear and computerised adaptive tests (CATs). A growing body of academic literature is available for researchers, however the practicalities of using MST for the design and delivery of real-world tests is still difficult to access. I hope this article provides a useful practical guide to test design and delivery.

The advantages of splitting tests up into units of items called *modules* and directing candidates to modules based on their latent ability scores, provides a more efficient (and shorter) test delivery than a corresponding linear test, while also offering an easier method of assembly, and greater protection against test copying.

I'll assume some familiarity with Item Response Theory (IRT) - and the Rasch model. Some knowledge of programming with R would be useful to understand the examples given. For background information and primers on these subjects I provide references.

My main focus is the development of tests for language learners, specifically English language learners. The theory and data processing involved can  be transferred to other domains that require measurement of latent ability. The IRT model that I focus on is the 1PL (or Rasch) model. For readers interested in extending analysis to more complex models, see [@magis2017computerized].

Developing a new test is a complex, multi-disciplinary task, involving specialist item writers, psychometricians, software engineers, validation experts and project managers. It is essential, before beginning any of this work to have a clear vision of what you are seeking to test, and the types of candidates that you will be targeting along with their needs. The main tasks when developing a new test wil include:

* Blueprint and test specification
* Test design
* Simulation
* Item writing
* Item Review and rating
* Pilot/Live Trial
* Rasch modelling and item banking
* Standard setting
* Live test

Once the live test has been deployed, continual monitoring and adjustment of the test will be essential to ensure that it remains secure and delivers on the targetted goals over the long term.

## Introduction to Item Response Theory

When you deliver a test to a set of candidates, how can you be sure that you are measuring those candidates in the same way, even though the test administrated to one candidate has different items from that administered to another candidate?

The underlying theoretical basis for MST is the branch of psychometrics known as Item Response Theory, or IRT. The theory proposes that there are *latent traits* (for example language ability) that can be measured using a set of tests delivered to the test candidate. These test items are typically scored as correct (1) or incorrect (0) - and it these dichotomous test types that we focus on here. The response to the test items can then be use to estimate the latent trait by fitting an IRT model. For MSTs, the test items are grouped into modules that can be delivered as separate but linked units in a test administration. Progression from module to module is dictated by the candidate's estimated ability, with cut-scores determining the next module to progress to.

### The Rasch Model

The simplest IRT model, the so-called *Rasch model*, applies logistic regression to items, using responses from a set of trial candidates. If you're familiar with linear regression, then the idea behind logistic regression is that the model fits a probability range (between 0.0 and 1.0) rather than a dependent variable. The independent variables are the difficulty of the item (usually designated as $b$), and the latent ability (which is denoted $\theta$). Because it only factors in the difficulty parameter of the item, it is also known as the one-parameter logistic model (1PL). Here's the *item response function* (IRF) that is used to model the probability of a correct response for candidate $i$ taking test item $j$:

$$
Pr(X_{ij} = 1|\theta_{i},b_{j}) = \frac{exp(\theta_{i} - b_{j})}{1+exp(\theta_{i} - b_{j})}
$$
$X$ is the item response (which is either 1 or 0 for the dichotomous tests we cover here), $\theta$ represents the latent ability being measured, and $b$ represents the difficulty of the item. Certain assumptions are made by the model, most importantly:

* Each item targets one and only one latent trait
* The items are independent of one another
* The response probability increases with the ability level using a logistic cumulative distribution function

This sigmoid function can be plotted as an Item Characteristic Curve (ICC). See below for three items of differing difficulties with ICCs plotted.

![Logistic Function](images/icc.png)

There is an equivalence between $\theta$ and $b$ such that a candidate with mean ability ($\theta = 0$) would be expected to get item 587 (blue) correct with a probability of 0.28. The less difficult item 583 (green) would have a probability of around 0.9 for a correct response for the same candidate. The right-most item, 868 (red) is the most difficult item.

An item that all candidates always get right because it is too easy, or they always get wrong because it's too hard has very little informative value for a test. It's telling you very little about the ability levels of your candidates. From this it follows that an item where half the candidates answer correct and the other half answer wrong has the most informational value; this is the point where $\theta$ and $b$ are equal. This relationship can be expressed by the Item Information Function (IIF). In the 1PL model the maximum value of the IIF is 0.25 (it can be different in the other models due to the influence of the $a$ parameter).

![Logistic Function](images/iif.png)


The function is expressed as:

$$
I_i(\theta, b_i) = P_i(\theta, b_i)Q_i(\theta, b_i)
$$

Greater information implies a smaller error of measurement. When designing a test it is important to deliver items with a range of difficulties that match the expected ability ranges so that you can accurately measure the your target candidates. In the above diagram you can see that item 587 (blue) is good at measuring candidates around ability level 0.95. Item 583 is better at measuring candidates with a lower ability level.

## IRT for Mulitstage Test Design

IRT is used to inform calibration of the test items after they've been developed by subject matter experts, as well as construction of the modules and routing between them. Scoring of the test can be derived from the $\theta$ estimates. Typically the initial module (known as the routing module) will have an overall difficulty of 0 (ie. a mix of easy and hard items), with subsequent routing directing candidates to easier or more difficult modules in order to 'hone in' on an accurate $\theta$ estimate at the end of the test.

A typical modular design will look like this:

![MST Design](images/modules.svg)

This is an example of a 1-2-4 design, but others are possible. Important questions to answer during test design are what module topology to use and what should be the overall difficulty levels for each module (represented as the mean item difficulty, $\bar{b}$).

In the pre-trial stage, it is possible to run simulations to help us answer these questions.

A benefit of using modules is that items can be placed into modules and then delivered as parallel test forms called *panels*. Having multiple panels helps to reduce item exposure rate as previously seen panels can be precluded from selection for future test administrations. It also makes it easier to assemble tests. It is possible to place items in more than one panel, but its important to ensure that parallel panels are of equivalent difficulty.

![Panels](images/panels.svg)

When constructed in this way each module can be considered as a module instance (for example Module A:3 being the instance of module A in panel 3),

## Data Sources and Management

In order to perform a simulated test, there are a number of different data sources that are required.

The first class of these data sources is the one that is used to store item information. The key entities to store here:

* Items
* Modules
* Panels

In addition to the item information, another data source is used for the candidate information. This forms the candidate information service (CIS) that will be used for the test. In addition to storing a unique candidate ID, additional metadata of interest, such as the candidate's length of study or first language (L1), may be held by the CIS.

Finally, test response data is required. Essentially this is a matrix containing rows of candidates and columns of items. Cells will contain a 1 (item answered correctly) or a 0 (item answered incorrectly). Items that were not presented (for example, from a module that was not in the test path for a candidate) are left blank and will be classed as NA.

This data may be stored as flat files (in CSV format for instance), or held in database tables, or in a mix of different sources. Typically you would want to keep them in a cloud storage system (for example in AWS S3 buckets or an RDS database) to ensure that the data is stored in a way that is secure, accessible and geographically distributed for resilience.

## MST Design Considerations

I mentioned earlier that multi-stage test designs comprise a number of modules arranged into stages. The question then becomes, how many stages do I need, and how many modules in each stage do I want? We saw earlier an example of a simple 1-2-4 design.

This design may not be sufficient for your needs. You need to consider whether there are groups of items that naturally fit into a particular stage (for example, seperating reading or listening tasks into seperate stages). You also need to consider how many items per stage you want to have. Also, in order for the routing between modules to produce acceptable results, you need to consider the relative difficulty of each module.

Normally the so-called routing module in stage 1 (Module A) would contain a mix of items with a mean difficulty of 0 (in other words 'medium' difficulty). Candidates that are placed in the lower ability range at the end of this module will be routed to the 'easy' module in stage 2 (Module B), and those in the upper ability range to the 'hard' module (Module C).

## Pre-Trial Simulation and Design

An initial simulation of your test to try out designs can be done by using the `mstR` library (see @magis2017computerized). Here the aim is to select a set of designs and see if there is an optimal design. It will also help understand how the scoring of the test should be implemented, including the number correct (NC) scores that will be used for routing, and the ability estimates for final scoring and reporting.

The `mstR` library can take an existing set of item responses, or you can use it to generate a set of randomised simulated item responses. This latter ability is useful in the pre-trial stage where we're interested in finding out how routing between modules should be established, or to validate assumptions in the test design.

The aim here is to sift several different test designs; and re-run simulations easily.

### Module Setup

With `mstR` we can define:

* the module topology (number of stages and modules per stage)
* the number of items per module and the module difficulty
* the IRT model to use

If we start out with the 1-2-4 design shown above, based on our test requirements, we can create a simultion for the test design with the following elements:

* a simulated item bank, `it.MST`
* the modules and the items they contain, `modules`
* the routing rules in the transition matrix `routing`

![MST Design](images/module-items.svg)

```{r module difficulties, echo=TRUE}
it.MST <- rbind(genDichoMatrix(12, model = "1PL", bPrior = c("norm", 0.0, 1)),
	genDichoMatrix(8, model = "1PL", bPrior = c("norm", -1.0, 1)),
	genDichoMatrix(8, model = "1PL", bPrior = c("norm", 1.0, 1)),
	genDichoMatrix(10, model = "1PL", bPrior = c("norm", -1.5, 1)),
	genDichoMatrix(10, model = "1PL", bPrior = c("norm", -0.5, 1)),
	genDichoMatrix(10, model = "1PL", bPrior = c("norm", 0.5, 1)),
	genDichoMatrix(10, model = "1PL", bPrior = c("norm", 1.5, 1)))
it.MST <- as.matrix(it.MST)
```

Here we've decided to use the 1PL (or Rasch) model, so we need to supply `bPrior` difficulty distributions, and convert the data frame into a matrix. We'll end up with a matrix of 68 items, each with randomised difficulty parameters. A test will comprise 30 items (12 + 8 + 10).

Selecting items into the modules is done by creating another matrix with 68 rows (items) and 7 columns (modules); a cell containing a 1 indicates membership of an item in a module.

```{r module items, echo=TRUE}
modules <- matrix(0, 68, 7)
modules[1:12, 1] <- 1
modules[13:20, 2] <- 1
modules[21:28, 3] <- 1
modules[29:38, 4] <- 1
modules[39:48, 5] <- 1
modules[49:58, 6] <- 1
modules[59:68, 7] <- 1
```

The final part of the design activity is construction of the routing rules, which is done by creating an $R \times R$ *transition matrix* which shows the valid paths to modules in the next stage and where $R$ is the number of modules. A value of 1 indicates that a path exists from module $i$ to module $j$ in the matrix.

```{r module routing, echo=TRUE}
routing <- matrix(0, 7, 7)
routing[1, 2:3] <- 1
routing[2, 4:6] <- 1
routing[3, 5:7] <- 1

cutoff <- matrix(NA, 7, 2)
cutoff[2,] <- c(-Inf, 0.0)
cutoff[3,] <- c(0.0, Inf)
cutoff[4,] <- c(-Inf, -1.0)
cutoff[5,] <- c(-1.0, 0.0)
cutoff[6,] <- c(0.0, 1.0)
cutoff[7,] <- c(1.0, Inf)

(cutoff)
```

As can be seen, modules 4-7 (Module D to Module G in the design) are final stage modules, so do not route onward.

Having created a simulated item bank for the design, it is possible to generate simulated item responses by selecting a $\theta$ for a simulated candidate:

```{r simulated test response, echo=TRUE}
theta <- 0.95

dfSampleResponse <- genPattern(theta, it.MST)

(dfSampleResponse)
```

Here we use a $\theta$ of `r theta`. Note that this doesn't really simulate a true MST test, as it ignores the routing rules and simply returns a response for all items in the simulated item bank. We can however use the `randomMST()` function to perform a more realistic simulation. The `randomMST()` function will generate a response pattern for a given simulated item bank, an MST structure, and a true ability level ($\theta$). We'll use maximum likelihood estimation (`"ML"`) for ability estimation, and the maximum Fisher information criterion (`"MFI"`) for module selection.

```{r simulated MST test response, echo=TRUE}
start <- list(theta = theta)
test <- list(method = "ML", moduleSelect = "MFI")
final <- list(method = "ML")

dfMSTResponse <- randomMST(trueTheta = theta, 
                           itemBank = it.MST, modules = modules, transMatrix = routing,
                           start = start, test = test, final = final)

(dfMSTResponse)
```

For this simluated candidate with a true $\theta$ = `r dfMSTResponse$trueTheta`, the estimated ability $\hat{\theta}$ = `r dfMSTResponse$thFinal`.

The Module Information Function is used to determine the 'best' (ie. the most informative) module to route to in the next stage. In this example the selected modules are `r dfMSTResponse$selected.modules`. A plot can be made as follows, showing the highlighted route through the multi-stage test:

```{r show simulated test, echo=FALSE}
plot(dfMSTResponse)
```

### Running Repeated Simulations

We can run repeated simulations varying the topology of the test (for example changing the 1-2-4 design presented earlier). Or by changing the number of items per module, the mean difficulty levels, or some other aspect of the model. By calculating the differences between estimated and true ability levels we can see which set of parameters is optimal.

In this example we'll change the number of items per module; keeping the total number constant, but changing the relative number of items as follows:

| Design | Stage 1 | Stage 2 | Stage 3 | Total |
| ------ | ------: | ------: | ------: | ----: |
| 1      | 12      | 8       | 10      | 30    |
| 2      | 12      | 10      | 8       | 30    |
| 3      | 10      | 12      | 8       | 30    |
| 4      | 10      | 8       | 12      | 30    |
| 5      | 8       | 10      | 12      | 30    |
| 6      | 8       | 12      | 10      | 30    |

The `mstDesign()` function is used to generate the set of test designs, by varying the number of items for each module by stage.

```{r MST designs}
mstDesign <- function(stage, difficulties) {
  matrixN <- stage[1] + (2 * stage[2]) + (4 * stage[3])
  modules <- matrix(0, matrixN, 7)
  moduleA <- stage[1]
  moduleB <- moduleA+stage[2]
  moduleC <- moduleB+stage[2]
  moduleD <- moduleC+stage[3]
  moduleE <- moduleD+stage[3]
  moduleF <- moduleE+stage[3]
  moduleG <- moduleF+stage[3]
  
  modules[1:moduleA, 1] <- 1
  modules[moduleA+1:stage[2], 2] <- 1
  modules[moduleB+1:stage[2], 3] <- 1
  modules[moduleC+1:stage[3], 4] <- 1  
  modules[moduleD+1:stage[3], 5] <- 1
  modules[moduleE+1:stage[3], 6] <- 1
  modules[moduleF+1:stage[3], 7] <- 1
  
  itembank <- rbind(genDichoMatrix(stage[1], model = "1PL", bPrior = c("norm", 0.0, 1)),
                    genDichoMatrix(stage[2], model = "1PL", bPrior = c("norm", -1.0, 1)),
                    genDichoMatrix(stage[2], model = "1PL", bPrior = c("norm", 1.0, 1)),
                    genDichoMatrix(stage[3], model = "1PL", bPrior = c("norm", -1.5, 1)),
                    genDichoMatrix(stage[3], model = "1PL", bPrior = c("norm", -0.5, 1)),
                    genDichoMatrix(stage[3], model = "1PL", bPrior = c("norm", 0.5, 1)),
                    genDichoMatrix(stage[3], model = "1PL", bPrior = c("norm", 1.5, 1)))
  itembank <- as.matrix(itembank)

  return(list("modules" = modules, "itemBank" = itembank))
}

stages <- list(c(12,8,10), c(12,10,8), c(10,12,8),
               c(10,8,12), c(8,10,12), c(8,12,10))
testDesigns <- lapply(stages, mstDesign)
```

To run the simulations we'll create a set of 8,550 simulated candidates with ability range -2.5 to 2.5 (stepped in increments of 0.25). Then we'll repeat for each of the six designs above, with the same item bank.

```{r setup simulation}
simulation.levels <- seq(-2.5, 2.0, 0.25) # 19 ability levels
simulation.candidates <- rep(simulation.levels, each = 450) # 19 * 450 = 8550 simulated candidates
simulation.responses <- matrix(NA, length(simulation.candidates), 6)
simulation.start <- list(theta = 0)
simulation.test <- list(method = "ML", moduleSelect = "MFI")
simulation.final <- list(method = "ML")
```

We'll then loop over each design, running the `randomMST()` function to simulate a test.

```{r simulate tests}
for (j in 1:6) {
  for (i in 1:length(simulation.candidates)) {
    prov <- randomMST(trueTheta = simulation.candidates[i],
      itemBank = testDesigns[[j]]$itemBank, modules = testDesigns[[j]]$modules,
      transMatrix = routing, genSeed = i,
      start = simulation.start, test = simulation.test, final = simulation.final)
    simulation.responses[i,j] <- prov$thFinal
  }
}
```

#### ASB and RMSE values

The results can now be graphed, with averaged signed bias (ASB) and root mean squared error (RMSE) values calculated for each design and for each ability level. ASB is the mean difference between estimated and true ability levels, and RMSE is the square root of the mean of the squared differences between estimated and true ability levels. With both measures, lower is better.

```{r simulation results}
simulation.ASB <- simulation.RMSE <- matrix(NA, length(simulation.levels), 6)
ASB <- function(t, th) mean(t-th)
RMSE <- function(t, th) sqrt(mean((t-th)^2))

for (i in 1:length(simulation.levels)) {
  for (j in 1:6) {
    index <- which(simulation.candidates == simulation.levels[i])
    simulation.ASB[i,j] <- ASB(simulation.responses[index,j], simulation.candidates[i])
    simulation.RMSE[i,j] <- RMSE(simulation.responses[index,j], simulation.candidates[i])
  }
}

dfASB <- data.frame(simulation.ASB)
dfASB$theta <- simulation.levels

dfRMSE <- data.frame(simulation.RMSE)
dfRMSE$theta <- simulation.levels
```

```{r simultation graphs}
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00")

ggplot(data = dfASB, aes(x = theta)) +
  scale_y_continuous(limits = c(-1, 5)) +
  geom_line(aes(y = X1, colour=cbPalette[1])) + 
  geom_line(aes(y = X2, colour=cbPalette[2])) +
  geom_line(aes(y = X3, colour=cbPalette[3])) +
  geom_line(aes(y = X4, colour=cbPalette[4])) +
  geom_line(aes(y = X5, colour=cbPalette[5])) +
  geom_line(aes(y = X6, colour=cbPalette[6])) +
  labs(x = "theta", y = "ASB")

ggplot(data = dfRMSE, aes(x = theta)) +
  scale_y_continuous(limits = c(-1, 5)) +
  geom_line(aes(y = X1, colour=cbPalette[1])) + 
  geom_line(aes(y = X2, colour=cbPalette[2])) +
  geom_line(aes(y = X3, colour=cbPalette[3])) +
  geom_line(aes(y = X4, colour=cbPalette[4])) +
  geom_line(aes(y = X5, colour=cbPalette[5])) +
  geom_line(aes(y = X6, colour=cbPalette[6])) +
  labs(x = "theta", y = "RMSE")
```

See chapter 7 of [@magis2017computerized] for details, or check out the [CRAN page](https://cran.r-project.org/web/packages/mstR/index.html) for `mstR`.

## Trialling and Test Validation

Once the parameters of the test design are well established, the test designers should have a good idea of the number of modules, items and relative difficulties required. 

### Item Writing and Intial Validation

A process of item writing will then be undertaken. Subject matter experts (SMEs) will be brought in to help define the instructions of test writers, and a set of items will then be created by the team of item writers. To take our test design example, we'll establish the number of items for each module that will be required at each CEFR level.

In general the test designers will want to ensure that each module contains a balance of items of varying difficulty. The Common European Frame of Reference (CEFR) is used to assign items into different ability levels.

![Module A Items](images/moduleA-items.svg)
A group of raters will be used to validate items and ensure that they are set at the correct ability level. Rating sheets are then collated and a decision then made on the CEFR ability rating for each item in the item bank. Various methods can be used to converge on an agreed ability level for each item; the simplest is to find the modal ability level (ie. the most frequently occurring) from all your raters. Where there is no single mode, or there are wide diagreements between raters, a manual adjustment to a commonly agreed ability level may be required. Ideally at least 5 raters would be used for each item to avoid this as far as possible.

Once ability levels are agreed, they can be used to update the item bank.

### Initial Trial

The purpose of the initial trial is to administer a test to a cohort of test candidates. Care should be taken to ensure that this cohort is representative of the target candidature. The test response can then be used to calculate item difficulties using the selected IRT model. We'll use the 1PL (Rasch) model in the examples that follow. 

#### Import and validation of trial data

The trial data is usually received as CSV or Excel files that comprise a candidate ID and then a set of responses (0 or 1) for each item. Not all items may have been taken by each candidate so there is a good chance of NA values occurring. Data validation and cleansing is a key part of preparing the data from the field trial so that it can be used in a Rasch analysis.

```{r}
dfTestResponses <- read_csv('trial-data.csv')

head(dfTestResponses[1:8])
```

The `ltm` library can be used to fit a number of dichotomous and polytomous IRT models. It's the fastest library that I've found for fitting a Rasch model. Before doing that we get some summary statistics from the data, including the Cronbach's alpha measure of covariance. 

#### Consistency and reliability

We want to make sure that our modules are internally consistent and reliable as tests of a given ability. In our test we've grouped various skills or abilities into the various stages. So, stage 1 (ie. Module A) is designed as a test of language use, stage 2 (Modules B and C) is designed to measure reading skills, and stage 3 is designed to measure listening skills. The overall test is designed to measure English language ability in non-native learners of English.

Cronbach's alpha is a function of the number of items in a test, the average covariance between pairs of items, and the variance of the total score. It is defined as:

$$
\alpha = \frac{p \times \bar{r}}{1 + (p – 1)\bar{r}}
$$

Here, $p$ is the number if items and $\bar{r}$ is the average of all covariances between items.

In our test we've split our items into different modules according to the skill that is being measured, so it makes sense to calculate alphas for each module, although we could also do so by stage if we wanted. We need to make sure that we remove any rows containing NA values before calculating Cronbach's alpha.

```{r Cronbachs alpha}
# drop the Candidate column, it's not required for the IRT functions
dfTrial <- dplyr::select(dfTestResponses, -Candidate)

# now create per-module item response data frames, remembering to remove 
# any candidates who did not take the module
dfModuleAItems <- dfTrial %>% dplyr::select(1:12) %>% na.omit()
dfModuleBItems <- dfTrial %>% dplyr::select(13:20) %>% na.omit()
dfModuleCItems <- dfTrial %>% dplyr::select(21:28) %>% na.omit()
dfModuleDItems <- dfTrial %>% dplyr::select(29:38) %>% na.omit()
dfModuleEItems <- dfTrial %>% dplyr::select(39:48) %>% na.omit()
dfModuleFItems <- dfTrial %>% dplyr::select(49:58) %>% na.omit()
dfModuleGItems <- dfTrial %>% dplyr::select(59:68) %>% na.omit()

# calculate Cronbach's alpha for each module
alphasModuleA <- ltm::cronbach.alpha(dfModuleAItems, standardized = TRUE, na.rm = TRUE)
alphasModuleB <- ltm::cronbach.alpha(dfModuleBItems, standardized = TRUE, na.rm = TRUE)
alphasModuleC <- ltm::cronbach.alpha(dfModuleCItems, standardized = TRUE, na.rm = TRUE)
alphasModuleD <- ltm::cronbach.alpha(dfModuleDItems, standardized = TRUE, na.rm = TRUE)
alphasModuleE <- ltm::cronbach.alpha(dfModuleEItems, standardized = TRUE, na.rm = TRUE)
alphasModuleF <- ltm::cronbach.alpha(dfModuleFItems, standardized = TRUE, na.rm = TRUE)
alphasModuleG <- ltm::cronbach.alpha(dfModuleGItems, standardized = TRUE, na.rm = TRUE)

# create a composite data frame and plot the alphas and the number of samples
# we'll want to focus on modules that show a low alpha and a small number of samples
# as we may want to get more trial data for these, or look at the items to make
# sure they are testing the right skill (or both)
module <- c('A', 'B', 'C', 'D', 'E', 'F', 'G')
stage <- c(1, 2, 2, 3, 3, 3, 3)
alpha <- c(alphasModuleA$alpha, alphasModuleB$alpha, alphasModuleC$alpha,
            alphasModuleD$alpha, alphasModuleE$alpha, alphasModuleF$alpha,
            alphasModuleG$alpha)
num_items <- c(alphasModuleA$p, alphasModuleB$p, alphasModuleC$p,
            alphasModuleD$p, alphasModuleE$p, alphasModuleF$p,
            alphasModuleG$p)
num_responses <- c(alphasModuleA$n, alphasModuleB$n, alphasModuleC$n,
            alphasModuleD$n, alphasModuleE$n, alphasModuleF$n,
            alphasModuleG$n)

dfCronbachsAlpha <- data.frame(module, stage, num_items, num_responses, alpha)

(dfCronbachsAlpha)

ggplot(dfCronbachsAlpha, aes(x=alpha, y=num_responses, size = num_items, color = module)) +
    geom_point() +
    scale_x_continuous(limits = c(0, 1)) +
    theme(legend.position = "right") +
    labs(x = "alpha", y = "# responses") +
    geom_vline(xintercept = 0.7, linetype="dotted")
```
Ideally we're looking for alphas of 0.7 and above; less than this indicates that there is weak covariance between the items in the module, which means they may not be testing the same ability. This measure is really only useful as a guide for further investigations; the ultimate decision on whether to modify the items in each module should be taken in collaboration with the raters and other subject matter experts.

```{r Rasch model}
# fit the Rasch model
raschmodel <- ltm::rasch(dfTrial, constraint = cbind(length(dfTrial) + 1, 1),
                     IRT.param = TRUE, start.val = "random")
rasch_summary <- summary(raschmodel)
```

#### Updating the item bank

Once difficulty coefficients have been calculated by fitting the Rasch model, the item bank should be updated to reflect this. The item difficulty ($b$ parameter) is a key component of the item, and should be stored along with the other information about the item. If a 2PL model is being used, then the infomation ($a$ parameter) would also be stored.

### Item Difficulty

The item difficulty parameters $b_{i}$ are returned and then output to show the spread of difficulties.

```{r Item difficulties}
df_difficulties <- rasch_summary$coefficients

df_displayDifficulties <- data.frame(
  item_id = str_extract(row.names(df_difficulties), "(Item[0-9]+)"),
  b = as.vector(df_difficulties[,1])
)
df_displayDifficulties <- df_displayDifficulties %>%
  filter(str_detect(item_id, "(Item[0-9]+)"))

# normalise difficulties that are outside normal ranges
df_displayDifficulties$b[df_displayDifficulties$b < -4] <- -4
df_displayDifficulties$b[df_displayDifficulties$b > 4] <- 4

ggplot(data = na.omit(df_displayDifficulties)) +
  geom_bar(mapping = aes(x = reorder(item_id, -b), y = b), stat = "identity", fill = "cornflowerblue") +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
  labs(x = "items", y = "Difficulty (b)")
```

### Banking the Updated Difficulty Parameters

Now that the model has been fitted, the difficulties derived from the Rasch model can be used to update the item bank.

```{r Item banking}
# output a CSV file with the item difficulties
write_csv(df_displayDifficulties, "item-difficulties.csv")
```

### Item Characteristic Curves (ICCs)

The logistic function that describes an item's likelihood of being answered correctly can be represented visually like this, using the `plot` function from the `ltm` package:

```{r}
itemNo <- 7
itemID <- str_extract(rownames(df_difficulties)[itemNo], "Item\\d+")
itemDifficulty <- df_difficulties[itemNo]
itemPlot <- data.frame(plot(raschmodel, type = "ICC", items = itemNo, plot = FALSE))
colnames(itemPlot) <- c('theta', 'probability')
ggplot(itemPlot, aes(x=theta, y=probability)) + 
  geom_line() +
  labs(title="Item Characteristic Curve", x="θ", y="P(X=1)")
```
This is the item characteristic curve (ICC), in this case for the seventh item in the trial data, `r itemID` with difficulty $b$ = `r itemDifficulty`.

### Module Characteristic Curves (MCCs)

You can sum the ICCs for all items in a module to get a Module Characteristic Curve (MCC) that expresses the difficulty for that module. These graphs show the relative difficulty of each module. They can be used to calibrate the modules so they are in line with the assumptions about module difficulty.

First we'll create a helper function called `trueScoreSummary` to help get the plot data from the `ltm` package into the format we can use for visualisations with ggplot.


```{r IRT helper functions}
# return a data frame with true score summaries
# that can then be plotted
trueScoreSummary <- function(plotData, module=NA) {
  data.frame(module = module, 
             theta = as.vector(plotData[,1]), 
             trueScore = as.vector(rowSums(plotData[,-1])))
}
```

We get the indices of the module A items and then create the plotting data before passing
this to ggplot.

```{r Panel 1, Stage 1 MCC}
moduleA1_items <- which(colnames(dfTrial) %in% colnames(dfModuleAItems))
plotData <- trueScoreSummary(plot(raschmodel, 
                                  type = "ICC", 
                                  items = moduleA1_items, plot = FALSE), 
                             "A")
ggplot(plotData, aes(x=theta, y=trueScore)) + 
  geom_line(aes(colour=module)) +
  labs(title="Module Characteristic Curve", x="θ", y="True Score")
```
You can read this off such that a candidate who scores 9 in the module A section of the test would have an estimated ability score, ot $\hat{\theta}$ of approximately 2.

We can compare the MCCs for the stage 2 modules:

```{r Panel 1, Stage 2 MCCs}
moduleB1_items <- which(colnames(dfTrial) %in% colnames(dfModuleBItems))
moduleC1_items <- which(colnames(dfTrial) %in% colnames(dfModuleCItems))

plotData <- rbind(
  trueScoreSummary(plot(raschmodel, type = "ICC", items = moduleB1_items, plot = FALSE), "B"),
  trueScoreSummary(plot(raschmodel, type = "ICC", items = moduleC1_items, plot = FALSE), "C")) 

ggplot(plotData, aes(x=theta, y=trueScore)) + 
  geom_line(aes(colour=module)) +
  labs(title="Module Characteristic Curve", x="θ", y="True Score")
```
A problem we can see already here is that these curves look too straight - ideally there should be a nice sigmoid shape to them. These MCCs have a low information content - in other words they are not very discriminating. We may want to move some items to provide a b etter shape to this curve.

And for the modules in stage 3:

```{r Panel 1, Stage 3 MCCs}
moduleD1_items <- which(colnames(dfTrial) %in% colnames(dfModuleDItems))
moduleE1_items <- which(colnames(dfTrial) %in% colnames(dfModuleEItems))
moduleF1_items <- which(colnames(dfTrial) %in% colnames(dfModuleFItems))
moduleG1_items <- which(colnames(dfTrial) %in% colnames(dfModuleGItems))

plotData <- rbind(
  trueScoreSummary(plot(raschmodel, type = "ICC", items = moduleD1_items, plot = FALSE), "D"),
  trueScoreSummary(plot(raschmodel, type = "ICC", items = moduleE1_items, plot = FALSE), "E"),
  trueScoreSummary(plot(raschmodel, type = "ICC", items = moduleF1_items, plot = FALSE), "F"),
  trueScoreSummary(plot(raschmodel, type = "ICC", items = moduleG1_items, plot = FALSE), "G")) 

ggplot(plotData, aes(x=theta, y=trueScore)) + 
  geom_line(aes(colour=module)) +
  labs(title="Module Characteristic Curve", x="θ", y="True Score")
```
This is already showing us something a little weird. Our D (very easy) module is actually a little bit more difficult than our F (moderately hard) module at the top end of the scale. And the E (moderately easy) module is more difficult than the G (very hard) module. All of the modules are quite similar in terms of difficulty (the MCCs are quite close to each other), and moreover the lines are quite flat - there is little discrimination between them. We'll see how this can be expressed more obivously in the next section.

### Item Information Functions (IIFs)



7. Summary

8. Bibliography


